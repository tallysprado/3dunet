{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6dc3d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import *\n",
    "import keras.backend as k\n",
    "import keras.utils\n",
    "from keras import optimizers as opt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import measure\n",
    "import math\n",
    "\n",
    "print(k.common.image_dim_ordering())\n",
    "\n",
    "k.common.set_image_dim_ordering('tf')\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d94763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    arrMin = np.amin(arr)\n",
    "    arrMax = np.amax(arr)\n",
    "    print(arrMin)\n",
    "    print(arrMax)\n",
    "    if arrMax != 0:\n",
    "        arr = np.subtract(arr,arrMin)\n",
    "        #print(arr)\n",
    "        arrMax = np.amax(arr)\n",
    "        arr = np.divide(arr,arrMax)\n",
    "        #print(arr)\n",
    "    else:\n",
    "        print(\"error, max value is zero\")\n",
    "    print(\"normalized\")\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe1e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar batch (entrada de cada iteração)\n",
    "import keras.utils\n",
    "import scipy.ndimage\n",
    "\n",
    "batch_size=3\n",
    "max_rotation_angle = 10\n",
    "max_shift = 0.2\n",
    "max_zoom = 0.2\n",
    "\n",
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 x_set,\n",
    "                 y_set,\n",
    "                 batch_size,\n",
    "                 image_dimensions=(128, 128, 128),\n",
    "                 shuffle=True,\n",
    "                 n_channels=1,\n",
    "                 n_classes=2):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_dimensions = image_dimensions\n",
    "        print(\"Generator created for image size: {}\".format(self.image_dimensions))\n",
    "        self.shuffle = shuffle\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.number_of_images = self.x.shape[0]\n",
    "        self.indices = np.arange(self.number_of_images)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "        #print(self.x.shape)\n",
    "        #print(self.y.shape)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.number_of_images / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.number_of_images)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        x = np.empty((self.batch_size, *self.image_dimensions, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.image_dimensions))\n",
    "        #print(batch_indices)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            flip_flag = np.random.randint(2)\n",
    "            #print(i)\n",
    "            if flip_flag == 1:\n",
    "                x[i,:,:,:,:] = np.flip(self.x[batch_indices[i],:,:,:,:], axis=0)\n",
    "                y[i,:,:,:]   = np.flip(self.y[batch_indices[i],:,:,:,0], axis=0)\n",
    "            else:\n",
    "                x[i,:,:,:,:] = self.x[batch_indices[i],:,:,:,:]\n",
    "                y[i,:,:,:]   = self.y[batch_indices[i],:,:,:,0]\n",
    "            \n",
    "        # Rotations\n",
    "        \n",
    "        x_rot = np.copy(x)\n",
    "        y_rot = np.copy(y)\n",
    "            \n",
    "        for i in range(self.batch_size):\n",
    "            #print(\"aug\",i)\n",
    "            angle_x = np.random.randint(-max_rotation_angle, max_rotation_angle)\n",
    "            x_rot[i,:,:,:,:] = scipy.ndimage.interpolation.rotate(\n",
    "                x[i,:,:,:,:], angle_x, (1,2), False, mode=\"constant\", cval=0, order=0)\n",
    "            y_rot[i,:,:,:] = scipy.ndimage.interpolation.rotate(\n",
    "                y[i,:,:,:], angle_x, (1,2), False, mode=\"constant\", cval=0, order=0)\n",
    "        \n",
    "            #angle_y = np.random.randint(-max_rotation_angle, max_rotation_angle)\n",
    "            #x_rot = scipy.ndimage.interpolation.rotate(x, angle_y, (0,2), False, mode=\"constant\", cval=0, order=0)\n",
    "            #y_rot = scipy.ndimage.interpolation.rotate(y, angle_y, (0,2), False, mode=\"constant\", cval=0, order=0)\n",
    "\n",
    "            #angle_z = np.random.randint(-max_rotation_angle, max_rotation_angle)\n",
    "            #x_rot = scipy.ndimage.interpolation.rotate(x, angle_z, (0,1), False, mode=\"constant\", cval=0, order=0)\n",
    "            #y_rot = scipy.ndimage.interpolation.rotate(y, angle_z, (0,1), False, mode=\"constant\", cval=0, order=0)\n",
    "        \n",
    "        # shift\n",
    "        \n",
    "        shift = np.random.uniform(-max_shift, max_shift, size=5)\n",
    "        shift[0] = 0.0\n",
    "        shift[4] = 0.0\n",
    "        # x_shift = scipy.ndimage.interpolation.shift(x_rot, shift)\n",
    "        # y_shift = scipy.ndimage.interpolation.shift(y_rot, shift[:4])\n",
    "        \n",
    "        # make sure values are between 0 and 1\n",
    "        \n",
    "        # x_aug = np.clip(x_shift, 0.0, 1.0)\n",
    "        # y_aug = np.clip(y_shift, 0.0, 1.0)\n",
    "        \n",
    "        x_aug = np.clip(x_rot, 0.0, 1.0)\n",
    "        y_aug = np.clip(y_rot, 0.0, 1.0)\n",
    "        \n",
    "        # convert segmentation to one-hot encoding\n",
    "        \n",
    "        y_onehot = keras.utils.to_categorical(y_aug, self.n_classes)\n",
    "\n",
    "        return x_aug, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690ecb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar rede\n",
    "num_classes = 2\n",
    "filter_multiplier = 20\n",
    "\n",
    "def nvidia_unet(input_size=128, num_classes=num_classes):\n",
    "    input_ = Input((input_size, input_size, input_size, 1))\n",
    "    skips = []\n",
    "    output = input_\n",
    "    c = num_classes\n",
    "    \n",
    "    num_layers = int(np.floor(np.log2(input_size)))\n",
    "    down_conv_kernel_sizes = np.zeros([num_layers], dtype=int)\n",
    "    down_filter_numbers = np.zeros([num_layers], dtype=int)\n",
    "    up_conv_kernel_sizes = np.zeros([num_layers], dtype=int)\n",
    "    up_filter_numbers = np.zeros([num_layers], dtype=int)\n",
    "    \n",
    "    for layer_index in range(num_layers):\n",
    "        down_conv_kernel_sizes[layer_index] = int(3)\n",
    "        down_filter_numbers[layer_index] = int( (layer_index + 1) * filter_multiplier + num_classes )\n",
    "        up_conv_kernel_sizes[layer_index] = int(4)\n",
    "        up_filter_numbers[layer_index] = int( (num_layers - layer_index - 1) * filter_multiplier + num_classes )\n",
    "    \n",
    "    print(\"Number of layers:       {}\".format(num_layers))\n",
    "    print(\"Filters in layers down: {}\".format(down_filter_numbers))\n",
    "    print(\"Filters in layers up:   {}\".format(up_filter_numbers))\n",
    "    \n",
    "    for shape, filters in zip(down_conv_kernel_sizes, down_filter_numbers):\n",
    "        skips.append(output)\n",
    "        output= Conv3D(filters, (shape, shape, shape), strides=2, padding=\"same\", activation=\"relu\")(output)\n",
    "        \n",
    "    for shape, filters in zip(up_conv_kernel_sizes, up_filter_numbers):\n",
    "        output = keras.layers.UpSampling3D()(output)\n",
    "        skip_output = skips.pop()\n",
    "        output = concatenate([output, skip_output], axis=4)\n",
    "\n",
    "        if filters != num_classes:\n",
    "            output = Conv3D(filters, (shape, shape, shape), activation=\"relu\", padding=\"same\")(output)\n",
    "            output = BatchNormalization(momentum=.9)(output)\n",
    "        else:\n",
    "            output = Conv3D(filters, (shape, shape, shape), activation=\"sigmoid\", padding=\"same\")(output)\n",
    "    \n",
    "    assert len(skips) == 0\n",
    "    return Model([input_], [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ab1d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 nrrds found\n",
      "4 downsized file paths created\n",
      "4 normalized file paths created\n"
     ]
    }
   ],
   "source": [
    "rootPath = \"/home/tallys/3dunet/\"\n",
    "nrrdPath = \"/home/tallys/3dunet/img/\"\n",
    "segPath = \"/home/tallys/3dunet/mask\"\n",
    "outputPath = \"/home/tallys/3dunet/output\"\n",
    "\n",
    "nrrdFilePaths = []\n",
    "nrrdFileNames = []\n",
    "preNormalizedFilePaths = []\n",
    "normalizedFilePaths = []\n",
    "normalizedFileNames = []\n",
    "\n",
    "labelFilePaths = []\n",
    "\n",
    "for root, dirs, files in os.walk(nrrdPath):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\"normalized.npy\") or filename.endswith(\"5dim.npy\"):\n",
    "                continue\n",
    "            elif filename.endswith(\".npy\"):\n",
    "                path=os.path.join(root,filename)\n",
    "                #print(\"found\",path)\n",
    "                a = np.load(path)\n",
    "                nrrdFilePaths.append(path)\n",
    "                #very messy, but is necessary to weed out the normalized images\n",
    "                if filename.endswith(\"normalized.npy\") or filename.endswith(\"5dim.npy\"):\n",
    "                    continue\n",
    "                elif filename.endswith(\".npy\"):\n",
    "                    noExtension = filename[:-4]\n",
    "                    nrrdFileNames.append(noExtension)\n",
    "                    normalizedFileNames.append(noExtension)\n",
    "                    newExtension = noExtension + '_normalized.npy'\n",
    "                    outPutFileName = os.path.join(root,newExtension)\n",
    "                    normalizedFilePaths.append(outPutFileName)\n",
    "                    \n",
    "                    preNewExtension = noExtension + '_pnormalized.npy'\n",
    "                    preOutputFileName = os.path.join(root,preNewExtension)\n",
    "                    preNormalizedFilePaths.append(preOutputFileName)\n",
    "                \n",
    "print(len(nrrdFilePaths),\"nrrds found\")\n",
    "print(len(preNormalizedFilePaths),\"downsized file paths created\")\n",
    "print(len(normalizedFilePaths),\"normalized file paths created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c357de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 downsized label paths created\n"
     ]
    }
   ],
   "source": [
    "downsizedLabelFilePaths=[]\n",
    "labelFilePaths=[]\n",
    "for root, dirs, files in os.walk(segPath):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\"Label.npy\"):                \n",
    "                noExtension = filename[:-4]\n",
    "                newExtension = noExtension + '_downsized.npy'\n",
    "                outPutFileName = os.path.join(root,newExtension)\n",
    "                downsizedLabelFilePaths.append(outPutFileName)\n",
    "                \n",
    "                path=os.path.join(root,filename)\n",
    "                labelFilePaths.append(path)\n",
    "print(len(downsizedLabelFilePaths),\"downsized label paths created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c4bc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 images skipped\n",
      "0 images downsized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "#preNormalized AKA downsized\n",
    "imageZResize = []\n",
    "nnnn=0\n",
    "skippedImageDownsizeCount=0\n",
    "imageDownsizeCount=0\n",
    "\n",
    "for filePath in preNormalizedFilePaths:\n",
    "    if path.exists(filePath):\n",
    "        skippedImageDownsizeCount+=1\n",
    "        \n",
    "    elif not path.exists(filePath):\n",
    "        file=nrrdFilePaths[nnnn]\n",
    "        print(file)\n",
    "        arr = np.load(file)\n",
    "        arr = zoom(arr, (0.25,0.25,0.25))\n",
    "        \n",
    "        shape=arr.shape\n",
    "        zAxis=(shape[0])\n",
    "        resize=128/zAxis\n",
    "        imageZResize.append(resize)\n",
    "        arr=zoom(arr, (resize,1,1))\n",
    "        print(arr.shape)\n",
    "        \n",
    "        np.save(filePath,arr)\n",
    "        imageDownsizeCount+=1\n",
    "    nnnn+=1\n",
    "\n",
    "print(skippedImageDownsizeCount,\"images skipped\")\n",
    "print(imageDownsizeCount,\"images downsized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15a4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 labels skipped\n",
      "0 labels downsized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "labelZResize=[]\n",
    "nnnnn=0\n",
    "skippedLabelDownsizeCount=0\n",
    "labelDownsizeCount=0\n",
    "\n",
    "for filePath in downsizedLabelFilePaths:\n",
    "    #print(e)\n",
    "    if path.exists(filePath):\n",
    "        skippedLabelDownsizeCount+=1\n",
    "        \n",
    "    elif not path.exists(filePath):\n",
    "        file=labelFilePaths[nnnnn]\n",
    "        print(file)\n",
    "        arr = np.load(file)\n",
    "        arr = zoom(arr, (0.25,0.25,0.25))\n",
    "        \n",
    "        shape=arr.shape\n",
    "        zAxis=(shape[0])\n",
    "        resize=128/zAxis\n",
    "        labelZResize.append(resize)\n",
    "        arr=zoom(arr, (resize,1,1))\n",
    "        print(arr.shape)\n",
    "        \n",
    "        np.save(filePath,arr)\n",
    "        labelDownsizeCount+=1\n",
    "    nnnnn+=1\n",
    "    \n",
    "print(skippedLabelDownsizeCount,\"labels skipped\")\n",
    "print(labelDownsizeCount,\"labels downsized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d4836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 scans skipped\n",
      "0 scans normalized\n"
     ]
    }
   ],
   "source": [
    "skippedNormalizedCount=0\n",
    "normalizedCount=0\n",
    "\n",
    "n=0\n",
    "for filePath in normalizedFilePaths:\n",
    "    #print(e)\n",
    "    if path.exists(filePath):\n",
    "        skippedNormalizedCount+=1\n",
    "        \n",
    "    elif not path.exists(filePath):\n",
    "        file=preNormalizedFilePaths[n]\n",
    "        arr = np.load(file)\n",
    "        normalized = normalize(arr)\n",
    "        np.save(filePath,normalized)\n",
    "        normalizedCount+=1\n",
    "    n+=1\n",
    "    \n",
    "print(skippedNormalizedCount,\"scans skipped\")\n",
    "print(normalizedCount,\"scans normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e44f5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2\n",
    "smooth = 1\n",
    "lam = 1\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = k.flatten(y_true)\n",
    "    y_pred_f = k.flatten(y_pred)\n",
    "    intersection = k.sum(y_true_f * y_pred_f)\n",
    "    \n",
    "    return (2 * intersection + smooth) / (k.sum(y_true_f) + k.sum(y_pred_f) + smooth)\n",
    "\n",
    "def custom_binary_crossentropy(y_true,y_pred):\n",
    "    #experimental binary crossentropy loss metric\n",
    "    #which takes into account the amount of islands\n",
    "    #not working and therefore\n",
    "    #unused at the time of writing\n",
    "    bc = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    islands = num_islands(bc,0)\n",
    "    loss = bc*(K.log(islands+eps))**lam\n",
    "    return (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac08afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 scans skipped\n",
      "0 scans given an extra dimension\n"
     ]
    }
   ],
   "source": [
    "image5Dim = []\n",
    "image5DimPaths = []\n",
    "imageSkippedCount = 0\n",
    "image5DimmedCount = 0\n",
    "\n",
    "n1=0\n",
    "\n",
    "for filePath in normalizedFilePaths:\n",
    "    filePath = filePath[:-4]\n",
    "    filePath = filePath + '_5dim.npy'\n",
    "    image5DimPaths.append(filePath)\n",
    "\n",
    "for filePath in image5DimPaths:\n",
    "    if path.exists(filePath):\n",
    "        arr=np.load(filePath)\n",
    "        image5Dim.append(arr)\n",
    "        imageSkippedCount+=1\n",
    "        \n",
    "    elif not path.exists(filePath):\n",
    "        file=normalizedFilePaths[n1]\n",
    "        print(file)\n",
    "        arr=np.load(file)\n",
    "        arr=arr[...,np.newaxis]\n",
    "        np.save(image5DimPaths[n1],arr)\n",
    "        \n",
    "        print(np.shape(arr))\n",
    "        image5Dim.append(arr)\n",
    "        image5DimmedCount+=1\n",
    "    n1+=1\n",
    "    \n",
    "print(imageSkippedCount,\"scans skipped\")\n",
    "print(image5DimmedCount,\"scans given an extra dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3392fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 labels skipped\n",
      "0 labels given an extra dimension\n"
     ]
    }
   ],
   "source": [
    "label5Dim = []\n",
    "label5DimPaths = []\n",
    "label5DimmedCount = 0\n",
    "labelSkippedCount = 0\n",
    "\n",
    "n2=0\n",
    "\n",
    "for filePath in downsizedLabelFilePaths:\n",
    "    filePath = filePath[:-4]\n",
    "    filePath = filePath + '_5dim.npy'\n",
    "    label5DimPaths.append(filePath)\n",
    "\n",
    "for filePath in label5DimPaths:\n",
    "    if path.exists(filePath):\n",
    "        arr=np.load(filePath)\n",
    "        label5Dim.append(arr)\n",
    "        labelSkippedCount+=1\n",
    "        \n",
    "    elif not path.exists(filePath):\n",
    "        file=downsizedLabelFilePaths[n2]\n",
    "        print(file)\n",
    "        arr=np.load(file)\n",
    "        arr=arr[...,np.newaxis]\n",
    "        np.save(label5DimPaths[n2],arr)\n",
    "        \n",
    "        print(np.shape(arr))\n",
    "        label5Dim.append(arr)\n",
    "        label5DimmedCount +=1\n",
    "    n2+=1\n",
    "\n",
    "print(labelSkippedCount,\"labels skipped\")\n",
    "print(label5DimmedCount,\"labels given an extra dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89762b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tallys/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (128,128,115,1) into shape (128,128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-95fb5decd60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage5Dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#size=np.ndarray(image5Dim).shape[1:4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage5Dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel5Dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (128,128,115,1) into shape (128,128)"
     ]
    }
   ],
   "source": [
    "size=np.array(image5Dim).shape[1:4]\n",
    "#size=np.ndarray(image5Dim).shape[1:4]\n",
    "X_train, X_test, y_train, y_test = train_test_split (image5Dim, label5Dim, test_size = 0.01)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fe173",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_learning_rate = 0.001\n",
    "min_learning_rate = 0.0001\n",
    "num_epochs = 20\n",
    "\n",
    "learning_rate_decay = (max_learning_rate - min_learning_rate) / num_epochs\n",
    "\n",
    "#set initial values as very easy to beat\n",
    "#so the first iteration will qualify for all of these\n",
    "prev_max_acc = np.full((num_epochs), 0, dtype=int)\n",
    "prev_min_acc = np.full((num_epochs), 1, dtype=int)\n",
    "\n",
    "prev_max_loss = np.full((num_epochs), 0, dtype=int)\n",
    "prev_min_loss = np.full((num_epochs), 1, dtype=int)\n",
    "accuracyLog = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tf.compat.v1.InteractiveSession.close(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33691e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range((len(image5Dim))):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "\n",
    "    k.common.set_image_dim_ordering('tf')\n",
    "    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "    session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "    X_test2 = image5Dim[iteration]\n",
    "    X_test2 = X_test2[np.newaxis,...]\n",
    "    newImage5Dim = np.array(np.delete(image5Dim,iteration,axis=0))\n",
    "    \n",
    "    y_test2 = label5Dim[iteration]\n",
    "    y_test2 = y_test2[np.newaxis,...]\n",
    "    newLabel5Dim = np.array(np.delete(label5Dim,iteration,axis=0))\n",
    "    \n",
    "    \n",
    "    trainingData = BatchGenerator(np.array(newImage5Dim),np.array(newLabel5Dim),image_dimensions=(size),batch_size=3)\n",
    "    validationData = BatchGenerator(np.array(X_test2),np.array(y_test2),image_dimensions=(size),batch_size=1)\n",
    "    \n",
    "    model = nvidia_unet(size[0], num_classes)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.adam(lr=max_learning_rate, decay=learning_rate_decay),\n",
    "               loss= \"binary_crossentropy\",\n",
    "               metrics=[dice_coef])\n",
    "    \n",
    "    history = model.fit_generator(trainingData,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2)\n",
    "    \n",
    "    score = model.evaluate_generator(validationData)\n",
    "    \n",
    "    print(score,\"on loop\",iteration)\n",
    "    \n",
    "    accuracyLog.append(score)\n",
    "    \n",
    "    accuracy = (score[1])\n",
    "    loss = (score[0])\n",
    "    \n",
    "    #this is purely for generation of graphs depicting the accuracy/loss trends\n",
    "    if history.history['dice_coef'][num_epochs-1] > prev_max_acc[num_epochs-1]:\n",
    "        prev_max_acc = history.history['dice_coef']\n",
    "        print(\"new max training accuracy:\",history.history['dice_coef'][num_epochs-1],\"max curve updated\")\n",
    "        \n",
    "    if history.history['dice_coef'][num_epochs-1] < prev_min_acc[num_epochs-1]:\n",
    "        prev_min_acc = history.history['dice_coef']\n",
    "        print(\"new min training accuracy:\",history.history['dice_coef'][num_epochs-1],\"min curve updated\")\n",
    "        \n",
    "    if history.history['loss'][num_epochs-1] > prev_max_loss[num_epochs-1]:\n",
    "        prev_max_loss = history.history['loss']\n",
    "        print(\"new max training loss:\",history.history['loss'][num_epochs-1],\"max loss curve updated\")\n",
    "        \n",
    "    if history.history['loss'][num_epochs-1] < prev_min_loss[num_epochs-1]:\n",
    "        prev_min_loss = history.history['loss']\n",
    "        print(\"new min training loss:\",history.history['loss'][num_epochs-1],\"min loss curve updated\")\n",
    "    \n",
    "    del history\n",
    "    del model\n",
    "    \n",
    "    #it occasionally does not clear on the first attempt\n",
    "    for e in range(5):\n",
    "        gc.collect()\n",
    "    k.clear_session()\n",
    "    tf.InteractiveSession.close(session)\n",
    "    \n",
    "    #while it clogs the output, this is done so that\n",
    "    #if memory runs out mid validation, it is easy to see\n",
    "    #the most up to date accuracy log\n",
    "    print(accuracyLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b25f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
